{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "f37f9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from src.utility.logger import append_log\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "7bdd9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates found: 0\n",
      "Total rows involved in duplication: 0\n",
      "Duplicates based on time: 0\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('data/raw/combined_iasi_no2_meteo_2020_2025_local.csv')\n",
    "\n",
    "# Check for exact duplicates across all columns\n",
    "exact_duplicates = df.duplicated().sum()\n",
    "print(f\"Exact duplicates found: {exact_duplicates}\")\n",
    "\n",
    "# View duplicate rows\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "print(f\"Total rows involved in duplication: {len(duplicate_rows)}\")\n",
    "\n",
    "# Check duplicates on specific key columns\n",
    "key_duplicates = df.duplicated(subset=['datetime']).sum()\n",
    "print(f\"Duplicates based on time: {key_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "91b1a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate key patterns:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicates (keep first occurrence)\n",
    "df_cleaned = df.drop_duplicates(keep='first')\n",
    "\n",
    "# For key-based duplicates, investigate first\n",
    "duplicate_keys = df[df.duplicated(subset=['datetime'], keep=False)]\n",
    "print(\"Duplicate key patterns:\")\n",
    "print(duplicate_keys.groupby(['datetime']).size().sort_values(ascending=False))\n",
    "\n",
    "# Remove duplicates based on key columns after investigation\n",
    "df_cleaned = df.drop_duplicates(subset=['datetime'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "793f43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Original dataset: {len(df)} rows\",\n",
    "        f\"Exact duplicates removed: {exact_duplicates}\",\n",
    "        f\"Key-based duplicates removed: {key_duplicates}\",\n",
    "        f\"Final dataset: {len(df_cleaned)} rows\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "17c85560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>sensors_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>value</th>\n",
       "      <th>temp_C</th>\n",
       "      <th>dewpoint_C</th>\n",
       "      <th>slp_hPa</th>\n",
       "      <th>wind_dir_deg</th>\n",
       "      <th>wind_speed_ms</th>\n",
       "      <th>precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22081.0</td>\n",
       "      <td>22081.0</td>\n",
       "      <td>22081.000000</td>\n",
       "      <td>22081.000000</td>\n",
       "      <td>22081.000000</td>\n",
       "      <td>48674.000000</td>\n",
       "      <td>48674.000000</td>\n",
       "      <td>48674.000000</td>\n",
       "      <td>48253.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>48674.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156800</td>\n",
       "      <td>27.574886</td>\n",
       "      <td>31.661637</td>\n",
       "      <td>119.342400</td>\n",
       "      <td>58.548198</td>\n",
       "      <td>10085.463286</td>\n",
       "      <td>31.269538</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>165.729178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>30.498318</td>\n",
       "      <td>97.136655</td>\n",
       "      <td>100.504995</td>\n",
       "      <td>1349.646573</td>\n",
       "      <td>101.462163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364.906361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156766</td>\n",
       "      <td>27.574866</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-168.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156766</td>\n",
       "      <td>27.574866</td>\n",
       "      <td>15.896116</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10122.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156766</td>\n",
       "      <td>27.574866</td>\n",
       "      <td>29.376155</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>10169.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156836</td>\n",
       "      <td>27.574908</td>\n",
       "      <td>43.917378</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>10226.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156836</td>\n",
       "      <td>27.574908</td>\n",
       "      <td>2217.676463</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>10474.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       location_id  sensors_id           lat           lon         value  \\\n",
       "count      22081.0     22081.0  22081.000000  22081.000000  22081.000000   \n",
       "mean        9369.0     28602.0     47.156800     27.574886     31.661637   \n",
       "std            0.0         0.0      0.000035      0.000021     30.498318   \n",
       "min         9369.0     28602.0     47.156766     27.574866     -1.000000   \n",
       "25%         9369.0     28602.0     47.156766     27.574866     15.896116   \n",
       "50%         9369.0     28602.0     47.156766     27.574866     29.376155   \n",
       "75%         9369.0     28602.0     47.156836     27.574908     43.917378   \n",
       "max         9369.0     28602.0     47.156836     27.574908   2217.676463   \n",
       "\n",
       "             temp_C    dewpoint_C       slp_hPa  wind_dir_deg  wind_speed_ms  \\\n",
       "count  48674.000000  48674.000000  48674.000000  48253.000000            6.0   \n",
       "mean     119.342400     58.548198  10085.463286     31.269538          -99.0   \n",
       "std       97.136655    100.504995   1349.646573    101.462163            0.0   \n",
       "min     -168.000000  -9999.000000  -9999.000000      1.000000          -99.0   \n",
       "25%       37.000000      0.000000  10122.000000     10.000000          -99.0   \n",
       "50%      116.000000     58.000000  10169.000000     26.000000          -99.0   \n",
       "75%      194.000000    124.000000  10226.000000     30.000000          -99.0   \n",
       "max      391.000000    236.000000  10474.000000    999.000000          -99.0   \n",
       "\n",
       "          precip_mm  \n",
       "count  48674.000000  \n",
       "mean     165.729178  \n",
       "std      364.906361  \n",
       "min        0.000000  \n",
       "25%        5.000000  \n",
       "50%        8.000000  \n",
       "75%        8.000000  \n",
       "max      999.000000  "
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "90905111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        location_id  sensors_id   location                   datetime  \\\n",
       "0              NaN         NaN        NaN  2020-01-01 02:00:00+02:00   \n",
       "1              NaN         NaN        NaN  2020-01-01 03:00:00+02:00   \n",
       "2              NaN         NaN        NaN  2020-01-01 04:00:00+02:00   \n",
       "3              NaN         NaN        NaN  2020-01-01 05:00:00+02:00   \n",
       "4              NaN         NaN        NaN  2020-01-01 06:00:00+02:00   \n",
       "...            ...         ...        ...                        ...   \n",
       "49213       9369.0     28602.0  IS-1-9369  2025-08-20 20:00:00+03:00   \n",
       "49214       9369.0     28602.0  IS-1-9369  2025-08-20 21:00:00+03:00   \n",
       "49215       9369.0     28602.0  IS-1-9369  2025-08-20 22:00:00+03:00   \n",
       "49216       9369.0     28602.0  IS-1-9369  2025-08-20 23:00:00+03:00   \n",
       "49217       9369.0     28602.0  IS-1-9369  2025-08-21 00:00:00+03:00   \n",
       "\n",
       "             lat        lon parameter  units      value  temp_C  dewpoint_C  \\\n",
       "0            NaN        NaN       NaN    NaN        NaN    45.0       -11.0   \n",
       "1            NaN        NaN       NaN    NaN        NaN    45.0        -5.0   \n",
       "2            NaN        NaN       NaN    NaN        NaN    47.0        -4.0   \n",
       "3            NaN        NaN       NaN    NaN        NaN    45.0        -2.0   \n",
       "4            NaN        NaN       NaN    NaN        NaN    44.0        -2.0   \n",
       "...          ...        ...       ...    ...        ...     ...         ...   \n",
       "49213  47.156836  27.574908       no2  µg/m³  33.613450     NaN         NaN   \n",
       "49214  47.156836  27.574908       no2  µg/m³  69.959590     NaN         NaN   \n",
       "49215  47.156836  27.574908       no2  µg/m³  88.330060     NaN         NaN   \n",
       "49216  47.156836  27.574908       no2  µg/m³  90.182830     NaN         NaN   \n",
       "49217  47.156836  27.574908       no2  µg/m³  42.274666     NaN         NaN   \n",
       "\n",
       "       slp_hPa  wind_dir_deg  wind_speed_ms sky_cover  precip_mm  \n",
       "0      10199.0          29.0            NaN         0        7.0  \n",
       "1      10202.0          30.0            NaN         0        7.0  \n",
       "2      10206.0          30.0            NaN         0        8.0  \n",
       "3      10209.0          31.0            NaN         0        8.0  \n",
       "4      10211.0          31.0            NaN         0        8.0  \n",
       "...        ...           ...            ...       ...        ...  \n",
       "49213      NaN           NaN            NaN       NaN        NaN  \n",
       "49214      NaN           NaN            NaN       NaN        NaN  \n",
       "49215      NaN           NaN            NaN       NaN        NaN  \n",
       "49216      NaN           NaN            NaN       NaN        NaN  \n",
       "49217      NaN           NaN            NaN       NaN        NaN  \n",
       "\n",
       "[49218 rows x 16 columns]>"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a67fa5",
   "metadata": {},
   "source": [
    "| Column       | Description                                                                 | Example value              |\n",
    "|--------------|-----------------------------------------------------------------------------|----------------------------|\n",
    "| location_id  | Unique numeric identifier of the monitoring location                        | 9369                       |\n",
    "| sensors_id   | Unique numeric identifier of the sensor within the location                 | 28602                      |\n",
    "| location     | Station code (often country code + site code + location ID)                 | RO0083A-9369               |\n",
    "| datetime     | Timestamp of measurement (ISO 8601 with timezone)                          | 2020-08-04T01:00:00+03:00  |\n",
    "| lat          | Latitude coordinate of the monitoring location                             | 47.1567664986992           |\n",
    "| lon          | Longitude coordinate of the monitoring location                            | 27.5748656243897           |\n",
    "| parameter    | Pollutant measured (e.g., `no2`, `pm10`, `pm25`, `o3`, etc.)               | no2                        |\n",
    "| units        | Units of measurement (varies by parameter)                                 | µg/m³                      |\n",
    "| value        | Recorded measurement value of the pollutant                                | 51.44521273                |\n",
    "\n",
    "\n",
    "| Column (raw)       | Description                                                                 | Units (raw)       | Missing code |\n",
    "|--------------------|-----------------------------------------------------------------------------|------------------|--------------|\n",
    "| year               | Year (4-digit)                                                             | YYYY             | –            |\n",
    "| month              | Month (2-digit)                                                            | MM               | –            |\n",
    "| day                | Day of month (2-digit)                                                     | DD               | –            |\n",
    "| hour               | Hour of day (UTC, 0–23)                                                     | HH               | –            |\n",
    "| air temperature    | Air temperature in **tenths of °C**                                         | 0.1 °C           | -9999        |\n",
    "| dew point temp     | Dew point temperature in **tenths of °C**                                   | 0.1 °C           | -9999        |\n",
    "| sea level pressure | Sea level pressure in **tenths of hPa**                                     | 0.1 hPa          | -9999        |\n",
    "| wind direction     | Wind direction from true north (0–360)                                      | degrees          | -999         |\n",
    "| wind speed         | Wind speed in **tenths of m/s**                                             | 0.1 m/s          | -9999        |\n",
    "| sky cover          | Cloud cover indicator (coded, e.g., oktas or station code dependent)        | categorical/code | -9999        |\n",
    "| precipitation      | Precipitation depth during the past hour                                    | mm               | -9999        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "eb31b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issues Found:\n",
      "  impossible_no2: 3930 records\n"
     ]
    }
   ],
   "source": [
    "def identify_quality_issues(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    issues = {}\n",
    "    \n",
    "    # Check for impossible values (domain-specific)\n",
    "    if 'temp_C' in df.columns:\n",
    "        impossible_temps = df[(df['temp_C'] < -500) | (df['temp_C'] > 700)]\n",
    "        issues['impossible_temperatures'] = len(impossible_temps)\n",
    "    \n",
    "    if 'value' in df.columns:\n",
    "        impossible_no2 = df[(df['value'] < 0 | (df['value'] > 300))]\n",
    "        issues['impossible_no2'] = len(impossible_no2)\n",
    "    \n",
    "    # Check for future dates\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        future_dates = df[df['timestamp'] > datetime.now()]\n",
    "        issues['future_dates'] = len(future_dates)\n",
    "    \n",
    "    # Check for format inconsistencies\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_patterns = df[col].astype(str).str.len().value_counts()\n",
    "        if len(unique_patterns) > 10:  # Many different lengths suggest format issues\n",
    "            issues[f'{col}_format_inconsistency'] = len(unique_patterns)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = identify_quality_issues(df_cleaned)\n",
    "print(\"Data Quality Issues Found:\")\n",
    "for issue, count in quality_report.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue}: {count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "1a83dade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>sensors_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>value</th>\n",
       "      <th>temp_C</th>\n",
       "      <th>dewpoint_C</th>\n",
       "      <th>slp_hPa</th>\n",
       "      <th>wind_dir_deg</th>\n",
       "      <th>wind_speed_ms</th>\n",
       "      <th>precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22081.0</td>\n",
       "      <td>22081.0</td>\n",
       "      <td>22081.000000</td>\n",
       "      <td>22081.000000</td>\n",
       "      <td>18151.000000</td>\n",
       "      <td>48669.000000</td>\n",
       "      <td>48640.000000</td>\n",
       "      <td>48443.000000</td>\n",
       "      <td>47734.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156800</td>\n",
       "      <td>27.574886</td>\n",
       "      <td>38.733436</td>\n",
       "      <td>119.364832</td>\n",
       "      <td>59.065399</td>\n",
       "      <td>10175.869269</td>\n",
       "      <td>20.747643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.938348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>29.164170</td>\n",
       "      <td>97.116429</td>\n",
       "      <td>77.021606</td>\n",
       "      <td>82.033924</td>\n",
       "      <td>10.640461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.411636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156766</td>\n",
       "      <td>27.574866</td>\n",
       "      <td>0.773662</td>\n",
       "      <td>-168.000000</td>\n",
       "      <td>-187.000000</td>\n",
       "      <td>9881.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156766</td>\n",
       "      <td>27.574866</td>\n",
       "      <td>23.577705</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10123.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156766</td>\n",
       "      <td>27.574866</td>\n",
       "      <td>33.887936</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>10169.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156836</td>\n",
       "      <td>27.574908</td>\n",
       "      <td>47.735446</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>10227.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9369.0</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>47.156836</td>\n",
       "      <td>27.574908</td>\n",
       "      <td>2217.676463</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>10474.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       location_id  sensors_id           lat           lon         value  \\\n",
       "count      22081.0     22081.0  22081.000000  22081.000000  18151.000000   \n",
       "mean        9369.0     28602.0     47.156800     27.574886     38.733436   \n",
       "std            0.0         0.0      0.000035      0.000021     29.164170   \n",
       "min         9369.0     28602.0     47.156766     27.574866      0.773662   \n",
       "25%         9369.0     28602.0     47.156766     27.574866     23.577705   \n",
       "50%         9369.0     28602.0     47.156766     27.574866     33.887936   \n",
       "75%         9369.0     28602.0     47.156836     27.574908     47.735446   \n",
       "max         9369.0     28602.0     47.156836     27.574908   2217.676463   \n",
       "\n",
       "             temp_C    dewpoint_C       slp_hPa  wind_dir_deg  wind_speed_ms  \\\n",
       "count  48669.000000  48640.000000  48443.000000  47734.000000            0.0   \n",
       "mean     119.364832     59.065399  10175.869269     20.747643            NaN   \n",
       "std       97.116429     77.021606     82.033924     10.640461            NaN   \n",
       "min     -168.000000   -187.000000   9881.000000      1.000000            NaN   \n",
       "25%       37.000000      0.000000  10123.000000     10.000000            NaN   \n",
       "50%      116.000000     58.000000  10169.000000     26.000000            NaN   \n",
       "75%      194.000000    124.000000  10227.000000     30.000000            NaN   \n",
       "max      391.000000    236.000000  10474.000000     36.000000            NaN   \n",
       "\n",
       "          precip_mm  \n",
       "count  40842.000000  \n",
       "mean       5.938348  \n",
       "std        2.411636  \n",
       "min        0.000000  \n",
       "25%        4.000000  \n",
       "50%        7.000000  \n",
       "75%        8.000000  \n",
       "max        9.000000  "
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.replace([-9999, -999, -99, 9999, 999], np.nan, inplace=True)\n",
    "df_cleaned.replace({'value': -1}, np.nan, inplace=True)\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "079ada92",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Replaced values with missing code with nan [-9999, -999, -99, 9999, 999]\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "96e3cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_id      float64\n",
      "sensors_id       float64\n",
      "location          object\n",
      "datetime          object\n",
      "lat              float64\n",
      "lon              float64\n",
      "parameter         object\n",
      "units             object\n",
      "value            float64\n",
      "temp_C           float64\n",
      "dewpoint_C       float64\n",
      "slp_hPa          float64\n",
      "wind_dir_deg     float64\n",
      "wind_speed_ms    float64\n",
      "sky_cover         object\n",
      "precip_mm        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "f97910cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "datetime64[ns, Europe/Bucharest]\n",
      "0   2020-01-01 02:00:00+02:00\n",
      "1   2020-01-01 03:00:00+02:00\n",
      "2   2020-01-01 04:00:00+02:00\n",
      "Name: datetime, dtype: datetime64[ns, Europe/Bucharest]\n"
     ]
    }
   ],
   "source": [
    "# Confirm that date time format is consistent\n",
    "print(df_cleaned[\"datetime\"].dtype)\n",
    "\n",
    "# Coerce to datetime with timezone awareness\n",
    "df_cleaned[\"datetime\"] = pd.to_datetime(df_cleaned[\"datetime\"], errors=\"coerce\", utc=True)\n",
    "# Convert from UTC to Iași local time (Europe/Bucharest)\n",
    "df_cleaned[\"datetime\"] = df_cleaned[\"datetime\"].dt.tz_convert(\"Europe/Bucharest\")\n",
    "\n",
    "print(df_cleaned[\"datetime\"].dtype)      # should show: datetime64[ns, Europe/Bucharest]\n",
    "print(df_cleaned[\"datetime\"].head(3))    # should print like: 2025-08-20 20:00:00+03:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "c1769231",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Original datetime type: {df[\"datetime\"].dtype}\",\n",
    "        f\"Final type: {df_cleaned[\"datetime\"].dtype}\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "8bc70935",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_dict = {\n",
    "    'location_id': 'int64',\n",
    "    'sensors_id': 'int64',\n",
    "    'location': 'category',\n",
    "    'lat': 'float64',\n",
    "    'lon': 'float64',\n",
    "    'parameter': 'category',\n",
    "    'units': 'category',\n",
    "    'value': 'float64',\n",
    "    'temp_C': 'float64',\n",
    "    'dewpoint_C': 'float64',\n",
    "    'slp_hPa': 'float64',\n",
    "    'wind_dir_deg': 'float64',\n",
    "    'wind_speed_ms': 'float64',\n",
    "    'sky_cover': 'category',\n",
    "    'precip_mm': 'float64'\n",
    "}\n",
    "\n",
    "for column, dtype in conversion_dict.items():\n",
    "    if column in df_cleaned.columns:\n",
    "        df_cleaned[column] = df_cleaned[column].astype(dtype, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "3a824766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_id                               float64\n",
      "sensors_id                                float64\n",
      "location                                 category\n",
      "datetime         datetime64[ns, Europe/Bucharest]\n",
      "lat                                       float64\n",
      "lon                                       float64\n",
      "parameter                                category\n",
      "units                                    category\n",
      "value                                     float64\n",
      "temp_C                                    float64\n",
      "dewpoint_C                                float64\n",
      "slp_hPa                                   float64\n",
      "wind_dir_deg                              float64\n",
      "wind_speed_ms                             float64\n",
      "sky_cover                                category\n",
      "precip_mm                                 float64\n",
      "dtype: object\n",
      "       location_id  sensors_id      location                  datetime  \\\n",
      "20000       9369.0     28602.0  RO0083A-9369 2022-04-15 01:00:00+03:00   \n",
      "20001       9369.0     28602.0  RO0083A-9369 2022-04-15 02:00:00+03:00   \n",
      "20002       9369.0     28602.0  RO0083A-9369 2022-04-15 03:00:00+03:00   \n",
      "20003       9369.0     28602.0  RO0083A-9369 2022-04-15 04:00:00+03:00   \n",
      "20004       9369.0     28602.0  RO0083A-9369 2022-04-15 05:00:00+03:00   \n",
      "\n",
      "             lat        lon parameter  units      value  temp_C  dewpoint_C  \\\n",
      "20000  47.156766  27.574866       no2  µg/m³  46.407045    65.0         1.0   \n",
      "20001  47.156766  27.574866       no2  µg/m³  38.815734    57.0         0.0   \n",
      "20002  47.156766  27.574866       no2  µg/m³  33.187330    47.0        -2.0   \n",
      "20003  47.156766  27.574866       no2  µg/m³  32.256013    20.0        -1.0   \n",
      "20004  47.156766  27.574866       no2  µg/m³  39.705857    12.0       -18.0   \n",
      "\n",
      "       slp_hPa  wind_dir_deg  wind_speed_ms sky_cover  precip_mm  \n",
      "20000  10224.0          29.0            NaN         0        0.0  \n",
      "20001  10223.0          30.0            NaN         0        0.0  \n",
      "20002  10221.0          29.0            NaN         0        0.0  \n",
      "20003  10221.0           NaN            NaN       0 -        NaN  \n",
      "20004  10215.0          12.0            NaN         5        0.0  \n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.dtypes)\n",
    "print(df_cleaned[20000:20005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "eeeede10",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Original data typs: {df.dtypes}\"\n",
    "        f\"Final data types: {df_cleaned.dtypes}\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "af41be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing air quality entries per year:\n",
      "year\n",
      "2020    5893\n",
      "2021    2082\n",
      "2022    6693\n",
      "2023    8684\n",
      "2024    3286\n",
      "2025     499\n",
      "dtype: int64\n",
      "Rows with present air quality entries per year:\n",
      "year\n",
      "2020    2879\n",
      "2021    6651\n",
      "2022    2024\n",
      "2024    5466\n",
      "2025    5061\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract year\n",
    "df_cleaned[\"year\"] = df_cleaned[\"datetime\"].dt.year\n",
    "\n",
    "# Count rows with missing air quality data per year\n",
    "missing_counts = (\n",
    "    df_cleaned[df_cleaned[\"location_id\"].isna()]\n",
    "    .groupby(\"year\")\n",
    "    .size()\n",
    ")\n",
    "\n",
    "present_counts = (\n",
    "    df_cleaned[df_cleaned[\"location_id\"].notna()]\n",
    "    .groupby(\"year\")\n",
    "    .size()\n",
    ")\n",
    "\n",
    "print(\"Rows with missing air quality entries per year:\")\n",
    "print(missing_counts)\n",
    "\n",
    "print(\"Rows with present air quality entries per year:\")\n",
    "print(present_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "afd875b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              value        temp_C    dewpoint_C       slp_hPa  wind_dir_deg  \\\n",
      "count  18151.000000  48669.000000  48640.000000  48443.000000  47734.000000   \n",
      "mean      38.733436    119.364832     59.065399  10175.869269     20.747643   \n",
      "std       29.164170     97.116429     77.021606     82.033924     10.640461   \n",
      "min        0.773662   -168.000000   -187.000000   9881.000000      1.000000   \n",
      "25%       23.577705     37.000000      0.000000  10123.000000     10.000000   \n",
      "50%       33.887936    116.000000     58.000000  10169.000000     26.000000   \n",
      "75%       47.735446    194.000000    124.000000  10227.000000     30.000000   \n",
      "max     2217.676463    391.000000    236.000000  10474.000000     36.000000   \n",
      "\n",
      "          precip_mm          year  \n",
      "count  40842.000000  49218.000000  \n",
      "mean       5.938348   2022.337092  \n",
      "std        2.411636      1.637435  \n",
      "min        0.000000   2020.000000  \n",
      "25%        4.000000   2021.000000  \n",
      "50%        7.000000   2022.000000  \n",
      "75%        8.000000   2024.000000  \n",
      "max        9.000000   2025.000000  \n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.drop(columns=[\"location_id\", \"sensors_id\", \"location\", \"lat\", \"lon\", \"parameter\", \"units\", \"wind_speed_ms\"])\n",
    "print(df_cleaned.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "74abf233",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Dropped columns that don't have variation location_id, sensors_id, location, lat, lon, parameter, units, wind_speed_ms]\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670b78d",
   "metadata": {},
   "source": [
    "Standard random train/test splitting is not appropriate for time-series data because it assumes that all observations are independent and identically distributed. In reality, air quality and meteorological data are sequential, and each observation is influenced by temporal patterns such as seasonality, daily cycles, or longer-term trends. If the data were shuffled randomly, the model could inadvertently use information from the future to predict the past, creating data leakage and artificially inflating performance metrics.\n",
    "\n",
    "To address this, the project uses TimeSeriesSplit, which preserves chronological order when creating training and testing datasets. In this approach, the model is always trained on earlier periods and tested on later ones, reflecting the real-world task of forecasting future air quality based on past conditions. Instead of relying on a single cut between train and test sets, TimeSeriesSplit generates multiple rolling splits. This makes it possible to evaluate model performance across different time periods, providing a more robust sense of how well the model generalizes.\n",
    "\n",
    "This method is particularly valuable for air quality forecasting in Iași, where non-stationary effects such as heating in winter or traffic intensity during specific months can significantly influence pollutant concentrations. By using a time-aware validation strategy, the project ensures that the evaluation reflects these real variations. Ultimately, this leads to more realistic and trustworthy forecasts, which are crucial if the model is to support sustainable urban planning and public health decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "36332249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_target(X: pd.DataFrame, y: pd.Series, name=\"y\"):\n",
    "    \"\"\"\n",
    "    Drop rows where the target is NaN; keep X/y aligned and report how many dropped.\n",
    "    Ensures y is numeric for f_regression.\n",
    "    \"\"\"\n",
    "    # ensure numeric target (coerce weird strings to NaN)\n",
    "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
    "\n",
    "    mask = y_num.notna()\n",
    "    dropped = (~mask).sum()\n",
    "\n",
    "    if dropped:\n",
    "        print(f\"[clean_target] Dropping {dropped} rows with NaN in {name}\")\n",
    "\n",
    "    X_clean = X.loc[mask]\n",
    "    y_clean = y_num.loc[mask]\n",
    "\n",
    "    return X_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "67b97541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Train: 0 to 16405\n",
      "  Test:  16406 to 32811\n",
      "Fold 2:\n",
      "  Train: 0 to 32811\n",
      "  Test:  32812 to 49217\n",
      "[clean_target] Dropping 7911 rows with NaN in fold_1_y_train\n",
      "[clean_target] Dropping 13347 rows with NaN in fold_1_y_test\n",
      "[clean_target] Dropping 21258 rows with NaN in fold_2_y_train\n",
      "[clean_target] Dropping 9809 rows with NaN in fold_2_y_test\n"
     ]
    }
   ],
   "source": [
    "# Create time series splits for cross-validation*\n",
    "X = df_cleaned.drop(columns=[\"value\"])  # features\n",
    "y = df_cleaned[\"value\"]\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "folds = {}\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X), start=1):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: {train_idx[0]} to {train_idx[-1]}\")\n",
    "    print(f\"  Test:  {test_idx[0]} to {test_idx[-1]}\")\n",
    "    folds[f\"fold_{i}\"] = {\n",
    "        \"X_train\": X.iloc[train_idx],\n",
    "        \"X_test\": X.iloc[test_idx],\n",
    "        \"y_train\": y.iloc[train_idx],\n",
    "        \"y_test\": y.iloc[test_idx],\n",
    "    }\n",
    "\n",
    "def drop_nan_target_in_folds(folds: dict, target_name=\"y\"):\n",
    "    for fold, d in folds.items():\n",
    "        Xtr, ytr = d[\"X_train\"], d[\"y_train\"]\n",
    "        Xte, yte = d[\"X_test\"],  d[\"y_test\"]\n",
    "\n",
    "        Xtr, ytr = clean_target(Xtr, ytr, name=f\"{fold}_y_train\")\n",
    "        Xte, yte = clean_target(Xte, yte, name=f\"{fold}_y_test\")\n",
    "\n",
    "        d[\"X_train\"], d[\"y_train\"] = Xtr, ytr\n",
    "        d[\"X_test\"],  d[\"y_test\"]  = Xte, yte\n",
    "    return folds\n",
    "\n",
    "folds = drop_nan_target_in_folds(folds, target_name=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "858ffe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Temporal split using TimeSeriesSplit with n_splits=5\",\n",
    "        f\"Dropping rows with nan values in target\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "9d25bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/pgy7xavqil7gwddr4blr2bkn5g508n6r-python3-3.13.4-env/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=20 is greater than n_features=18. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- datetime feature extractor (unchanged) ---\n",
    "def make_dt_block(datetime_col=\"datetime\"):\n",
    "    def _extract(X: pd.DataFrame):\n",
    "        dt = pd.to_datetime(X[datetime_col])\n",
    "        return pd.DataFrame({\n",
    "            \"dt__hour\": dt.dt.hour,\n",
    "            \"dt__dow\": dt.dt.dayofweek,\n",
    "            \"dt__month\": dt.dt.month,\n",
    "            \"dt__is_weekend\": (dt.dt.dayofweek >= 5).astype(int),\n",
    "        }, index=X.index)\n",
    "\n",
    "    def _feature_names(transformer, input_features=None):\n",
    "        # must match the columns returned by _extract\n",
    "        return np.array([\"dt__hour\", \"dt__dow\", \"dt__month\", \"dt__is_weekend\"])\n",
    "\n",
    "    return FunctionTransformer(\n",
    "        _extract,\n",
    "        feature_names_out=_feature_names\n",
    "    )\n",
    "\n",
    "# --- clip target values at 300 (handles sensor malfunctions) ---\n",
    "def clip_target(y, min_val=0, max_val=300):\n",
    "    \"\"\"\n",
    "    Clip target values to a specified range.\n",
    "    \"\"\"\n",
    "    return y.clip(lower=min_val, upper=max_val)\n",
    "\n",
    "# CORRECT: Learn all parameters from training data only\n",
    "def create_preprocessing_pipeline(X_train, y_train, k=20):\n",
    "    \"\"\"Create preprocessing pipeline fitted on training data\"\"\"\n",
    "\n",
    "    # Ensure numeric y and drop rows with NaN target (required by f_regression)\n",
    "    y_train = pd.to_numeric(y_train, errors=\"coerce\")\n",
    "    y_train = clip_target(y_train, max_val=300)\n",
    "    mask = y_train.notna()\n",
    "    if not mask.any():\n",
    "        raise ValueError(\"No non-NaN targets in y_train.\")\n",
    "    X_train = X_train.loc[mask]\n",
    "    y_train = y_train.loc[mask]\n",
    "\n",
    "    # Identify numeric vs categorical columns (excluding datetime)\n",
    "    numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    categorical_features = X_train.select_dtypes(include=[\"object\", \"category\", \"string\"]).columns\n",
    "\n",
    "    # Numeric pipeline: impute with median, then scale\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Categorical pipeline: impute with most_frequent, then one-hot encode\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    # Datetime pipeline: extract (hour/dow/month/is_weekend) then scale\n",
    "    dt_pipeline = Pipeline(steps=[\n",
    "        (\"dt_extract\", make_dt_block(\"datetime\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Combine\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipeline, numeric_features),\n",
    "            (\"cat\", categorical_pipeline, categorical_features),\n",
    "            (\"dt\",  dt_pipeline, [\"datetime\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Add feature selection at the end\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"selector\", SelectKBest(score_func=f_regression, k=k))\n",
    "    ])\n",
    "\n",
    "    # Fit pipeline on training data\n",
    "    X_train_processed = pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "    return X_train_processed, pipeline\n",
    "\n",
    "def apply_preprocessing_pipeline(X_test, pipeline):\n",
    "    \"\"\"\n",
    "    Apply the fitted preprocessing pipeline to test data.\n",
    "    `pipeline` is the object returned by create_preprocessing_pipeline(...).\n",
    "    \"\"\"\n",
    "    return pipeline.transform(X_test)\n",
    "\n",
    "for fold_name, data in folds.items():\n",
    "        X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "        X_test, y_test   = data[\"X_test\"], data[\"y_test\"]\n",
    "\n",
    "        # Fit preprocessing on training\n",
    "        X_train_proc, preprocessors = create_preprocessing_pipeline(X_train, y_train)\n",
    "        # Apply to test\n",
    "        X_test_proc = apply_preprocessing_pipeline(X_test, preprocessors)\n",
    "\n",
    "        # Store results back into dict\n",
    "        data[\"X_train_proc\"] = X_train_proc\n",
    "        data[\"X_test_proc\"]  = X_test_proc\n",
    "        data[\"y_train\"] = clip_target(pd.to_numeric(data[\"y_train\"], errors=\"coerce\"), max_val=300)\n",
    "        data[\"y_test\"]  = clip_target(pd.to_numeric(data[\"y_test\"],  errors=\"coerce\"), max_val=300)\n",
    "        data[\"preprocessors\"] = preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "d26f24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_log(\n",
    "    \"outputs/logs/data_cleaning_log.txt\",\n",
    "    [\n",
    "        f\"Preprocessed datasets using preprocessing pipeline, for categorical and numerical features\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "c57718b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fold_1 with headers to data/processed\n",
      "Saved fold_2 with headers to data/processed\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names(pipeline):\n",
    "    try:\n",
    "        # Features after preprocessing (numeric + one-hot expanded)\n",
    "        names = pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "        # Apply SelectKBest mask if present\n",
    "        if \"selector\" in pipeline.named_steps:\n",
    "            support = pipeline.named_steps[\"selector\"].get_support()\n",
    "            names = names[support]\n",
    "        return list(names)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "out_dir = Path(\"data/processed\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fold_name, data in folds.items():\n",
    "    pipeline = data.get(\"preprocessors\") or data.get(\"pipeline\")\n",
    "    feature_names = get_feature_names(pipeline)\n",
    "\n",
    "    # Wrap arrays with headers if available\n",
    "    X_train_df = pd.DataFrame(data[\"X_train_proc\"], columns=feature_names)\n",
    "    X_test_df  = pd.DataFrame(data[\"X_test_proc\"],  columns=feature_names)\n",
    "\n",
    "    # Targets as Series with header\n",
    "    y_train_s  = pd.Series(data[\"y_train\"]).reset_index(drop=True)\n",
    "    y_test_s   = pd.Series(data[\"y_test\"]).reset_index(drop=True)\n",
    "    y_train_s.name = \"target\"\n",
    "    y_test_s.name  = \"target\"\n",
    "\n",
    "    # Save\n",
    "    X_train_df.to_csv(out_dir / f\"{fold_name}_X_train.csv\", index=False)\n",
    "    X_test_df.to_csv(out_dir / f\"{fold_name}_X_test.csv\", index=False)\n",
    "    y_train_s.to_csv(out_dir / f\"{fold_name}_y_train.csv\", index=False, header=True)\n",
    "    y_test_s.to_csv(out_dir / f\"{fold_name}_y_test.csv\", index=False, header=True)\n",
    "\n",
    "    print(f\"Saved {fold_name} with headers to {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b799525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
